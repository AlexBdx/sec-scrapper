{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running python 3.7.3 (>= python 3.6)\n"
     ]
    }
   ],
   "source": [
    "from secScraper import *\n",
    "import sys\n",
    "\n",
    "if sys.version_info[0] < 3 or sys.version_info[1] < 6:\n",
    "    raise Exception(\"Must be using Python >= 3.6 due to reliance on ordered default dict.\")\n",
    "else:\n",
    "    version = \"[INFO] Running python {}.{}.{}\".format(*sys.version_info[:3])\n",
    "    if display.run_from_ipython():\n",
    "        %load_ext autoreload\n",
    "        %autoreload 2\n",
    "        %matplotlib notebook\n",
    "        version += \" for ipython\" if display.run_from_ipython() else \"\"\n",
    "    print(\"[INFO] Running python {}.{}.{} (>= python 3.6)\".format(*sys.version_info[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import psycopg2\n",
    "import ast\n",
    "import copy\n",
    "\n",
    "# Spark\n",
    "# import findspark\n",
    "# findspark.init('/home/alex/spark-2.4.4-bin-hadoop2.7')\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the nb of processes to use based on cmd line arguments/setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if display.run_from_ipython():\n",
    "    nb_processes_requested = mp.cpu_count()  # From IPython, fixed setting\n",
    "    # nb_processes_requested = 1 # From IPython, fixed setting\n",
    "else:\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-p\", \"--processes\", type=int, default=mp.cpu_count(), help=\"Number of processes launched to process the reports.\")\n",
    "    args = vars(ap.parse_args())\n",
    "    nb_processes_requested = args[\"processes\"]\n",
    "    if not 1 <= nb_processes_requested <= mp.cpu_count():\n",
    "        raise ValueError('[ERROR] Number of processes requested is incorrect.\\\n",
    "                         \\n{} CPUs are available on this machine, please select a number of processes between 1 and {}'\n",
    "                         .format(mp.cpu_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser(\"~\")\n",
    "_s = {\n",
    "    'path_stage_1_data': os.path.join(home, 'Desktop/filtered_text_data/nd_data/'),\n",
    "    'path_stock_database': os.path.join(home, 'Desktop/Insight project/Database/Ticker_stock_price.csv'),\n",
    "    'path_filtered_stock_data': os.path.join(home, 'Desktop/Insight project/Database/filtered_stock_data.csv'),\n",
    "    'path_stock_indexes': os.path.join(home, 'Desktop/Insight project/Database/Indexes/'),\n",
    "    'path_filtered_index_data': os.path.join(home, 'Desktop/Insight project/Database/Indexes/filtered_index_data.csv'),\n",
    "    'path_lookup': os.path.join(home, 'Desktop/Insight project/Database/lookup.csv'),\n",
    "    'path_filtered_lookup': os.path.join(home, 'Desktop/Insight project/Database/filtered_lookup.csv'),\n",
    "    'path_master_dictionary': os.path.join(home, 'Desktop/Insight project/Database/LoughranMcDonald_MasterDictionary_2018.csv'),\n",
    "    'path_dump_crsp': os.path.join(home, 'Desktop/Insight project/Database/dump_crsp_merged.txt'),\n",
    "    'path_output_folder': os.path.join(home, 'Desktop/Insight project/Outputs'),\n",
    "    'path_dump_cik_scores': os.path.join(home, 'Desktop/Insight project/Outputs/dump_cik_scores.csv'),\n",
    "    'path_dump_pf_values': os.path.join(home, 'Desktop/Insight project/Outputs/dump_pf_values.csv'),\n",
    "    'path_dump_master_dict': os.path.join(home, 'Desktop/Insight project/Outputs/dump_master_dict.csv'),\n",
    "    'metrics': ['diff_jaccard', 'diff_sk_cosine_tf_idf', 'diff_gfg_editDistDP'],\n",
    "    'stop_words': False,\n",
    "    'lemmatize': False,\n",
    "    'differentiation_mode': 'quarterly',\n",
    "    'pf_balancing': 'unbalanced',\n",
    "    'time_range': [(2012, 1), (2018, 4)],\n",
    "    'bin_count': 5,\n",
    "    'tax_rate': 0,\n",
    "    'histogram_date_span_ratio': 0.5,\n",
    "    'report_type': ['10-K', '10-Q'],\n",
    "    'sections_to_parse_10k': [],\n",
    "    'sections_to_parse_10q': [],\n",
    "    'type_daily_price': 'closing'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_s['pf_init_value'] = 100.0  # In points\n",
    "_s['epsilon'] = 0.001  # Rounding error\n",
    "# Calculated settings\n",
    "_s['list_qtr'] = qtrs.create_qtr_list(_s['time_range'])\n",
    "\n",
    "if _s['bin_count'] == 5:\n",
    "    _s['bin_labels'] = ['Q'+str(n) for n in range(1, _s['bin_count']+1)]\n",
    "elif _s['bin_count'] == 10:\n",
    "    _s['bin_labels'] = ['D'+str(n) for n in range(1, _s['bin_count']+1)]\n",
    "else:\n",
    "    raise ValueError('[ERROR] This type of bin has not been implemented yet.')\n",
    "\n",
    "# Create diff metrics and sing metrics\n",
    "_s['diff_metrics'] = [m for m in _s['metrics'] if m[:4] == 'diff']\n",
    "_s['sing_metrics'] = [m for m in _s['metrics'] if m[:4] == 'sing']\n",
    "# Reports considered to calculate the differences\n",
    "if _s['differentiation_mode'] == 'quarterly':\n",
    "    _s['lag'] = 1\n",
    "    _s['sections_to_parse_10k'] = ['1a', '3', '7', '7a', '9a']\n",
    "    _s['sections_to_parse_10q'] = ['_i_2', '_i_3', '_i_4', 'ii_1', 'ii_1a']\n",
    "elif _s['differentiation_mode'] == 'yearly':\n",
    "    _s['lag'] = 4\n",
    "    _s['sections_to_parse_10k'] = ['1a', '3', '7', '7a', '9a']\n",
    "    _s['sections_to_parse_10q'] = ['_i_2', '_i_3', '_i_4', 'ii_1', 'ii_1a']\n",
    "\n",
    "_s['common_quarterly_sections'] = {\n",
    "        '10-K': ['1a', '3', '7', '7a', '9a'],\n",
    "        '10-Q': ['ii_1a', 'ii_1', '_i_2', '_i_3', '_i_4']\n",
    "}  # Exhibits are not taken into account\n",
    "\"\"\"_s['common_yearly_sections'] = {\n",
    "    '10-K': ['1', '1a', '1b', '2', '3', '4', '5', '6', '7', '7a', '8', '9', '9a', '9b', '10', '11', '12', '13', '14', '15'],\n",
    "    '10-Q': ['_i_1', '_i_2', '_i_3', '_i_4', 'ii_1', 'ii_1a', 'ii_2', 'ii_3', 'ii_4', 'ii_5', 'ii_6']\n",
    "}\"\"\"\n",
    "_s['common_yearly_sections'] = {\n",
    "    '10-K': ['7'],\n",
    "    '10-Q': ['_i_2']\n",
    "}  # Take into account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer s to a read only dict\n",
    "read_only_dict = pre_processing.ReadOnlyDict()\n",
    "for key in _s:  # Brute force copy\n",
    "    read_only_dict[key] = _s[key]\n",
    "s = read_only_dict  # Copy back\n",
    "s.set_read_state(read_only=True)  # Set as read only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load external tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = psycopg2.connect(host=\"localhost\", dbname=\"postgres\", user=\"postgres\", password=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deleted table settings\n",
      "[INFO] Creating the following table:\n",
      "CREATE TABLE settings(IDX integer PRIMARY KEY,KEY text,VALUE text)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 1381.24it/s]\n"
     ]
    }
   ],
   "source": [
    "postgres.settings_to_postgres(connector, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the list of CIK for which we have complete data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem in our case is that we have 3 different database to play with:\n",
    "1. The SEC provides information based on the CIK of the entity\n",
    "2. Given that the CIK is used by no one else, we use a lookup table to transform that into tickers. But we do not have all the correspondances, so the list of useful CIK is shrunk.\n",
    "3. Finally, we only have stock prices for so many tickers. So that shrinks the CIK list even further.\n",
    "\n",
    "We end up with a reduced list of CIK that we can play with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the sentiment analysis dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ...Loading Master Dictionary 85000\n",
      "Master Dictionary loaded from file: \n",
      "  /home/alex/Desktop/Insight project/Database/LoughranMcDonald_MasterDictionary_2018.csv\n",
      "  86,486 words loaded in master_dictionary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm_dictionary = Load_MasterDictionary.load_masterdictionary(s['path_master_dictionary'], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all the unique CIK from the SEC filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 395,773 10-X\n",
      "[INFO] Shrunk to 350,283 ['10-K', '10-Q']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 85259/350283 [00:00<00:00, 852580.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 18,009 unique CIK in master index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350283/350283 [00:00<00:00, 836589.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cik_path contains data on 18,009 CIK numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cik_path = pre_processing.load_cik_path(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the largest {CIK: ticker} possible given our lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM lookup;\n",
      "[INFO] Loaded 13,738 CIK/Tickers correspondances.\n"
     ]
    }
   ],
   "source": [
    "lookup, reverse_lookup = postgres.retrieve_lookup(connector)\n",
    "print(\"[INFO] Loaded {:,} CIK/Tickers correspondances.\".format(len(lookup)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Intersected SEC & lookup.\n",
      "cik_path: 9,547 CIK | lookup: 9,547 CIK\n"
     ]
    }
   ],
   "source": [
    "cik_path, lookup = pre_processing.intersection_sec_lookup(cik_path, lookup)\n",
    "print(\"[INFO] Intersected SEC & lookup.\")\n",
    "print(\"cik_path: {:,} CIK | lookup: {:,} CIK\"\n",
    "      .format(len(cik_path), len(lookup)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stock data and drop all CIKs for which we don't have data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM stock_data;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4769013/4769013 [00:05<00:00, 879379.18it/s] \n"
     ]
    }
   ],
   "source": [
    "# Load all stock prices\n",
    "stock_data = postgres.retrieve_all_stock_data(connector, 'stock_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3181\n",
      "[INFO] Intersected lookup & stock data.\n",
      "lookup: 3,290 tickers | stock_data: 3,181 tickers\n"
     ]
    }
   ],
   "source": [
    "lookup, stock_data = pre_processing.intersection_lookup_stock(lookup, stock_data)\n",
    "print(\"[INFO] Intersected lookup & stock data.\")\n",
    "print(\"lookup: {:,} tickers | stock_data: {:,} tickers\"\n",
    "      .format(len(lookup.values()), len(stock_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stock indexes - will serve as benchmark later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26373/26373 [00:00<00:00, 1772613.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM index_data;\n",
      "[INFO] Loaded the following index data: ['IXIC', 'SPX', 'DJI', 'RUT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index_data = postgres.retrieve_all_stock_data(connector, 'index_data')\n",
    "print(\"[INFO] Loaded the following index data:\", list(index_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagate these intersection all the way to cik_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, we have just done it for lookup. So we only need to re-run an intersection for lookup and sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Intersected SEC & lookup.\n",
      "cik_path: 3,290 CIK | lookup: 3,290 CIK\n"
     ]
    }
   ],
   "source": [
    "cik_path, lookup = pre_processing.intersection_sec_lookup(cik_path, lookup)\n",
    "print(\"[INFO] Intersected SEC & lookup.\")\n",
    "print(\"cik_path: {:,} CIK | lookup: {:,} CIK\"\n",
    "      .format(len(cik_path), len(lookup)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, cik_path and lookup should have the same number of keys as the CIK is unique in the path database.\n",
    "\n",
    "However, multiple CIK can redirect to the same ticker if the company changed its ticker over time. That should be a very limited amount of cases though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cik_path.keys() == lookup.keys()\n",
    "assert len(set(lookup.values())) == len(set(stock_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At that point, we have a {CIK: ticker} for which the stock is known, which will enable comparison and all down the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review all CIKs: make sure there is only one submission per quarter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the goal is to build a list of CIK that will successfully be parsed for the time_range considered.\n",
    "It should be trivial for a vast majority of the CIK, but ideally there should be only one document per quarter for each CIK from the moment they are listed to the moment they are delisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3290/3290 [00:00<00:00, 4275.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] 957 CIKs caused trouble\n",
      "[INFO] Removed all the CIK that did not have one report per quarter.\n",
      "cik_dict: 2,333 CIK\n"
     ]
    }
   ],
   "source": [
    "# Create the list of quarters to consider\n",
    "cik_path = pre_processing.review_cik_publications(cik_path, s)\n",
    "print(\"[INFO] Removed all the CIK that did not have one report per quarter.\")\n",
    "print(\"cik_dict: {:,} CIK\".format(len(cik_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] We are left with 2,333 CIKs that meet our requirements:\n",
      "- The ticker can be looked up in the CIK/ticker table\n",
      "- The stock data is available for that ticker\n",
      "- There is one and only one report per quarter\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] We are left with {:,} CIKs that meet our requirements:\".format(len(cik_path)))\n",
    "print(\"- The ticker can be looked up in the CIK/ticker table\")\n",
    "print(\"- The stock data is available for that ticker\")\n",
    "print(\"- There is one and only one report per quarter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump all the data to postgres\n",
    "This is done so that the Flask webapp can retrieve the settings that were used at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613\n"
     ]
    }
   ],
   "source": [
    "print(list(cik_path.keys()).index(10456))  # Find BAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connector = psycopg2.connect(host=\"localhost\", dbname=\"postgres\", user=\"postgres\", password=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postgres.settings_to_postgres(connector, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "header_lookup = (('CIK', 'integer'), ('TICKER', 'text'))\n",
    "postgres.lookup_to_postgres(connector, lookup, header_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "header = (('TICKER', 'text'), ('TIMESTAMP', 'date'), \n",
    "          ('ASK', 'float'), ('MARKET_CAP', 'float'))\n",
    "path = os.path.join(home, 'Desktop/Insight project/Database/stock_data_filtered.csv')\n",
    "postgres.stock_data_csv_to_postgres(connector, path, header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stock_data_2 = postgres.retrieve_stock_data(connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of CIK that should make it until the end. It is time to open the relevant reports and start parsing. This step takes a lot of time and can get arbitrarily long as the metrics get fancier.\n",
    "\n",
    "You do not want to keep in RAM all the parsed data. However, there are only ~100 quarters for which we have data and the stage 2 files are no more than 1 Mb in size (Apple seems to top out at ~ 325 kb). So 100 Mb per core + others, that's definitely doable. More cores will use more RAM, but the usage remains reasonable.\n",
    "\n",
    "We use multiprocessing to go through N CIK at once but a single core is dedicated to going through a given CIK for the specified time_range. Such a core can be running for a while if the company has been in business for the whole time_range and publish a lot of text data in its 10-K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "# nb_processes_requested = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting a pool of 8 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:15<12:20, 15.10s/it]\n",
      "  4%|▍         | 2/50 [00:16<08:42, 10.89s/it]\n",
      "2it [00:15, 10.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] list index out of range in parser.clean_first_markers (10-Q)\n",
      "This is the res\n",
      " {'_i_1': [(6476, 6498)], '_i_2': [(29598, 29622)], '_i_3': [(64460, 64485)], '_i_4': [(65862, 65884)], 'ii_1': [(67300, 67318)], 'ii_1a': [], 'ii_2': [(67891, 67916)], 'ii_4': [(68040, 68057)], 'ii_5': [(68088, 68110)], 'ii_6': [(68175, 68197)]}\n",
      "[WARNING] /home/alex/Desktop/filtered_text_data/nd_data/2012/QTR2/20120504_10-Q_edgar_data_1171486_0001193125-12-210830_1.txt failed parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:18<06:29,  8.30s/it]\n",
      "  8%|▊         | 4/50 [00:20<04:53,  6.38s/it]\n",
      " 10%|█         | 5/50 [00:26<04:44,  6.33s/it]\n",
      " 12%|█▏        | 6/50 [00:27<03:28,  4.73s/it]\n",
      " 14%|█▍        | 7/50 [00:32<03:29,  4.86s/it]\n",
      "7it [00:32,  4.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Here is full_sect: |[]|\n",
      "[ERROR] Original res: {'_i_1': [], '_i_2': [], '_i_3': [], '_i_4': [], 'ii_1': [], 'ii_1a': [], 'ii_2': [], 'ii_3': [], 'ii_4': [], 'ii_5': [], 'ii_6': []}\n",
      "[WARNING] /home/alex/Desktop/filtered_text_data/nd_data/2017/QTR2/20170509_10-Q_edgar_data_1032208_0000086521-17-000033_1.txt failed parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:34<02:42,  3.86s/it]\n",
      " 18%|█▊        | 9/50 [00:35<02:02,  2.99s/it]\n",
      " 20%|██        | 10/50 [00:38<02:08,  3.21s/it]\n",
      " 22%|██▏       | 11/50 [00:40<01:40,  2.58s/it]\n",
      " 24%|██▍       | 12/50 [00:41<01:24,  2.22s/it]\n",
      " 26%|██▌       | 13/50 [00:45<01:41,  2.74s/it]\n",
      "13it [00:45,  2.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] list index out of range in parser.clean_first_markers (10-Q)\n",
      "This is the res\n",
      " {'_i_1': [(4105, 4128)], '_i_2': [(35180, 35202)], '_i_3': [(50226, 50249)], '_i_4': [(52879, 52899)], 'ii_1a': [], 'ii_6': [(53608, 53629)]}\n",
      "[WARNING] /home/alex/Desktop/filtered_text_data/nd_data/2012/QTR2/20120509_10-Q_edgar_data_712770_0001104659-12-035186_1.txt failed parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [00:47<01:36,  2.68s/it]\n",
      "14it [00:47,  2.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Here is full_sect: |[]|\n",
      "[ERROR] Original res: {'1': [], '1a': [], '1b': [], '2': [], '3': [], '4': [], '5': [], '6': [], '7': [], '7a': [], '8': [], '9': [], '9a': [], '9b': [], '10': [], '11': [], '12': [], '13': [], '14': [], '15': []}\n",
      "[WARNING] /home/alex/Desktop/filtered_text_data/nd_data/2017/QTR1/20170221_10-K_edgar_data_98362_0000098362-17-000031_1.txt failed parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:50<01:36,  2.75s/it]\n",
      " 32%|███▏      | 16/50 [00:57<02:14,  3.97s/it]\n",
      " 34%|███▍      | 17/50 [01:01<02:05,  3.81s/it]\n",
      " 36%|███▌      | 18/50 [01:02<01:41,  3.18s/it]\n",
      "18it [01:02,  3.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] list index out of range in parser.clean_first_markers (10-K)\n",
      "This is the res\n",
      " {'1': [(8756, 8774), (155992, 156010)], '1a': [(192356, 192371)], '2': [], '3': [(184747, 184766)], '5': [(185903, 185923)], '9a': [(325397, 325416)], '9b': [(327113, 327129)], '10': [(327179, 327203)], '11': [(328109, 328129)], '12': [(328475, 328494)], '13': [(329505, 329523)], '14': [(329774, 329794)], '15': [(330089, 330112)]}\n",
      "[WARNING] /home/alex/Desktop/filtered_text_data/nd_data/2018/QTR1/20180221_10-K_edgar_data_1278021_0001564590-18-002713_1.txt failed parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [01:05<01:33,  3.00s/it]\n",
      " 40%|████      | 20/50 [01:07<01:26,  2.87s/it]\n",
      " 42%|████▏     | 21/50 [01:12<01:33,  3.23s/it]\n",
      " 44%|████▍     | 22/50 [01:12<01:11,  2.54s/it]\n",
      " 46%|████▌     | 23/50 [01:22<02:02,  4.52s/it]\n",
      " 48%|████▊     | 24/50 [01:23<01:32,  3.55s/it]\n",
      "24it [01:23,  3.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Here is full_sect: |[]|\n",
      "[ERROR] Original res: {'_i_1': [], '_i_2': [], '_i_3': [], '_i_4': [], 'ii_1': [], 'ii_1a': [], 'ii_2': [], 'ii_3': [], 'ii_4': [], 'ii_5': [], 'ii_6': []}\n",
      "[WARNING] /home/alex/Desktop/filtered_text_data/nd_data/2012/QTR2/20120501_10-Q_edgar_data_82020_0001104659-12-031348_1.txt failed parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [01:25<01:18,  3.14s/it]\n",
      " 52%|█████▏    | 26/50 [01:28<01:16,  3.21s/it]\n",
      " 54%|█████▍    | 27/50 [01:30<01:01,  2.67s/it]\n",
      " 56%|█████▌    | 28/50 [01:37<01:29,  4.07s/it]\n",
      " 58%|█████▊    | 29/50 [01:46<01:55,  5.50s/it]\n",
      " 60%|██████    | 30/50 [01:51<01:45,  5.25s/it]\n",
      " 62%|██████▏   | 31/50 [01:53<01:22,  4.37s/it]\n",
      " 64%|██████▍   | 32/50 [01:56<01:09,  3.86s/it]\n",
      " 66%|██████▌   | 33/50 [01:57<00:50,  2.98s/it]\n",
      "33it [01:56,  2.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Here is full_sect: |[]|\n",
      "[ERROR] Original res: {'_i_1': [], '_i_2': [], '_i_3': [], '_i_4': [], 'ii_1': [], 'ii_1a': [], 'ii_2': [], 'ii_3': [], 'ii_4': [], 'ii_5': [], 'ii_6': []}\n",
      "[WARNING] /home/alex/Desktop/filtered_text_data/nd_data/2012/QTR2/20120420_10-Q_edgar_data_1065088_0001065088-12-000037_1.txt failed parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [01:59<00:44,  2.77s/it]\n",
      "34it [01:59,  2.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] list index out of range in parser.clean_first_markers (10-K)\n",
      "This is the res\n",
      " {'1': [(9333, 9353)], '1a': [(25142, 25160)], '1b': [(65382, 65406)], '3': [(251900, 251916)], '5': [], '6': [(87118, 87138)], '7': [(88778, 88800)], '7a': [(267461, 267486)], '8': [(272349, 272370)], '9': [(431923, 431942)], '9a': [(432047, 432068)], '9b': [(434449, 434467)], '10': [(434573, 434595)], '11': [(435174, 435196)], '12': [(435591, 435612)], '13': [(436081, 436101)], '14': [(436471, 436493)], '15': [(436898, 436919)]}\n",
      "[WARNING] /home/alex/Desktop/filtered_text_data/nd_data/2012/QTR1/20120312_10-K_edgar_data_1056903_0001104659-12-017424_1.txt failed parsing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [02:01<00:40,  2.72s/it]\n",
      " 72%|███████▏  | 36/50 [02:07<00:50,  3.63s/it]\n",
      " 74%|███████▍  | 37/50 [02:11<00:49,  3.82s/it]\n",
      " 76%|███████▌  | 38/50 [02:13<00:36,  3.08s/it]\n",
      " 78%|███████▊  | 39/50 [02:23<00:55,  5.08s/it]\n",
      " 80%|████████  | 40/50 [02:24<00:38,  3.86s/it]\n",
      " 82%|████████▏ | 41/50 [02:26<00:31,  3.48s/it]\n",
      " 84%|████████▍ | 42/50 [02:35<00:40,  5.00s/it]\n",
      " 86%|████████▌ | 43/50 [02:38<00:31,  4.47s/it]\n",
      " 88%|████████▊ | 44/50 [02:39<00:21,  3.56s/it]\n",
      " 90%|█████████ | 45/50 [02:40<00:13,  2.65s/it]\n",
      " 92%|█████████▏| 46/50 [02:41<00:09,  2.28s/it]\n",
      " 94%|█████████▍| 47/50 [02:43<00:06,  2.13s/it]\n",
      " 96%|█████████▌| 48/50 [02:45<00:04,  2.09s/it]\n",
      " 98%|█████████▊| 49/50 [02:58<00:05,  5.23s/it]\n",
      "100%|██████████| 50/50 [02:58<00:00,  3.69s/it]\n",
      "50it [02:57,  3.69s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 42 CIK were successfully processed - 8/50 CIK failed.\n",
      "Detailed stats and error codes: [42, 8, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Processing the reports will be done in parrallel in a random order\n",
    "# Settings in s are cast to dict for pickling - the custom class is not supported\n",
    "nb_cik_to_process = len(cik_path.keys())\n",
    "nb_cik_to_process = 50\n",
    "cik_path = {k: cik_path[k] for k in cik_path.keys() if k in list(cik_path.keys())[:nb_cik_to_process]}\n",
    "# cik_path = {k: cik_path[k] for k in cik_path.keys() if k in list(cik_path.keys())}\n",
    "\n",
    "# print(list(cik_path.keys()).index(10456))  # Find BAX\n",
    "cik_scores = {k: 0 for k in cik_path.keys()}  # Organized by ticker\n",
    "data_to_process = ([k, v, {**s}, lm_dictionary] for k, v in cik_path.items())\n",
    "assert cik_path.keys() == cik_scores.keys()\n",
    "#print(data_to_process)\n",
    "#result = process_cik(data_to_process[0])\n",
    "#cik_perf[result[0]] = result[1]\n",
    "#print(cik_perf)\n",
    "#assert 0\n",
    "processing_stats = [0, 0, 0, 0, 0, 0]\n",
    "#qtr_metric_result = {key: [] for key in s['list_qtr']}\n",
    "if nb_processes_requested > 1:\n",
    "    with mp.Pool(processes=nb_processes_requested) as p:\n",
    "    #with mp.Pool(processes=min(mp.cpu_count(), 1)) as p:\n",
    "        print(\"[INFO] Starting a pool of {} workers\".format(nb_processes_requested))\n",
    "\n",
    "        with tqdm(total=nb_cik_to_process) as pbar:\n",
    "            for i, value in tqdm(enumerate(p.imap_unordered(processing.process_cik, data_to_process))):\n",
    "                pbar.update()\n",
    "                #qtr = list_qtr[i]\n",
    "                # Each quarter gets a few metrics\n",
    "                if value[1] == {}:\n",
    "                    # The parsing failed\n",
    "                    del cik_scores[value[0]]\n",
    "                else:\n",
    "                    cik_scores[value[0]] = value[1]\n",
    "                processing_stats[value[2]] += 1\n",
    "\n",
    "elif nb_processes_requested == 1:\n",
    "    print(\"[INFO] Running on {} core (multiprocessing is off)\".format(nb_processes_requested))\n",
    "    # print(list(data_to_process))\n",
    "    with tqdm(total=nb_cik_to_process) as pbar:\n",
    "        for i, value in tqdm(enumerate(map(processing.process_cik, data_to_process))):\n",
    "            pbar.update()\n",
    "            #qtr = list_qtr[i]\n",
    "            # Each quarter gets a few metrics\n",
    "            if value[1] == {}:\n",
    "                # The parsing failed\n",
    "                del cik_scores[value[0]]\n",
    "            else:\n",
    "                cik_scores[value[0]] = value[1]\n",
    "            processing_stats[value[2]] += 1\n",
    "\n",
    "elif nb_processes_requested == 0:\n",
    "    # Spark mode!!\n",
    "    print(\"[INFO] Running with Spark\")\n",
    "    sc = pyspark.SparkContext(appName=\"model_calculations\")\n",
    "    print(\"[INFO] Context started\")\n",
    "    spark_result = sc.parallelize(data_to_process).map(processing.process_cik)\n",
    "    spark_result = spark_result.take(nb_cik_to_process)\n",
    "    sc.stop()\n",
    "    \n",
    "    # Process the result\n",
    "    with tqdm(total=nb_cik_to_process) as pbar:\n",
    "        for i, value in tqdm(enumerate(spark_result)):\n",
    "            pbar.update()\n",
    "            #qtr = list_qtr[i]\n",
    "            # Each quarter gets a few metrics\n",
    "            if value[1] == {}:\n",
    "                # The parsing failed\n",
    "                del cik_scores[value[0]]\n",
    "            else:\n",
    "                cik_scores[value[0]] = value[1]\n",
    "            processing_stats[value[2]] += 1\n",
    "           \n",
    "        #qtr_metric_result[value['0']['qtr']] = value\n",
    "\n",
    "print(\"[INFO] {} CIK were successfully processed - {}/{} CIK failed.\".format(len(cik_scores), len(cik_path)-len(cik_scores), len(cik_path)))\n",
    "print(\"Detailed stats and error codes:\", processing_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing - Welcome to the gettho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flip the result dictionary to present a per qtr view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 821.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] There is no stock data for 712771 during (2014, 3)\n",
      "[WARNING] There is no stock data for 1376339 during (2012, 2)\n",
      "[WARNING] There is no stock data for 1179755 during (2017, 2)\n",
      "[WARNING] There is no stock data for 352363 during (2012, 2)\n",
      "[WARNING] There is no stock data for 1097864 during (2015, 3)\n",
      "Unique cik {712771, 1097864, 1179755, 352363, 1376339}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metric_scores = post_processing.create_metric_scores(cik_scores, lookup, stock_data, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of companies that do not have data for a given qtr.\n",
      "This is because they are listed later in the time_range\n",
      "(2012, 2) 9/42\n",
      "(2012, 3) 7/42\n",
      "(2012, 4) 10/42\n",
      "(2013, 1) 10/42\n",
      "(2013, 2) 10/42\n",
      "(2013, 3) 8/42\n",
      "(2013, 4) 7/42\n",
      "(2014, 1) 5/42\n",
      "(2014, 2) 6/42\n",
      "(2014, 3) 7/42\n",
      "(2014, 4) 8/42\n",
      "(2015, 1) 8/42\n",
      "(2015, 2) 9/42\n",
      "(2015, 3) 10/42\n",
      "(2015, 4) 10/42\n",
      "(2016, 1) 11/42\n",
      "(2016, 2) 11/42\n",
      "(2016, 3) 12/42\n",
      "(2016, 4) 13/42\n",
      "(2017, 1) 14/42\n",
      "(2017, 2) 15/42\n",
      "(2017, 3) 16/42\n",
      "(2017, 4) 16/42\n",
      "(2018, 1) 16/42\n",
      "(2018, 2) 16/42\n",
      "(2018, 3) 16/42\n",
      "(2018, 4) 18/42\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Number of companies that do not have data for a given qtr.\")\n",
    "print(\"This is because they are listed later in the time_range\")\n",
    "for qtr in s['list_qtr'][s['lag']:]:\n",
    "    print(qtr, \"{}/{}\".format(len([cik for cik in metric_scores['diff_jaccard'][qtr] \n",
    "                    if metric_scores['diff_jaccard'][qtr][cik] == {}]), len(cik_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = post_processing.metrics_correlation(metric_scores, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_jaccard</th>\n",
       "      <th>diff_sk_cosine_tf_idf</th>\n",
       "      <th>diff_gfg_editDistDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006886</td>\n",
       "      <td>0.111210</td>\n",
       "      <td>0.050776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.400043</td>\n",
       "      <td>0.575123</td>\n",
       "      <td>0.568553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020048</td>\n",
       "      <td>0.178910</td>\n",
       "      <td>0.079579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.378089</td>\n",
       "      <td>0.808978</td>\n",
       "      <td>0.610323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.216978</td>\n",
       "      <td>0.324798</td>\n",
       "      <td>0.304664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diff_jaccard  diff_sk_cosine_tf_idf  diff_gfg_editDistDP\n",
       "0      0.006886               0.111210             0.050776\n",
       "1      0.400043               0.575123             0.568553\n",
       "2      0.020048               0.178910             0.079579\n",
       "3      0.378089               0.808978             0.610323\n",
       "4      0.216978               0.324798             0.304664"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_jaccard</th>\n",
       "      <th>diff_sk_cosine_tf_idf</th>\n",
       "      <th>diff_gfg_editDistDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>diff_jaccard</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929317</td>\n",
       "      <td>0.963715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diff_sk_cosine_tf_idf</th>\n",
       "      <td>0.929317</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diff_gfg_editDistDP</th>\n",
       "      <td>0.963715</td>\n",
       "      <td>0.925329</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       diff_jaccard  diff_sk_cosine_tf_idf  \\\n",
       "diff_jaccard               1.000000               0.929317   \n",
       "diff_sk_cosine_tf_idf      0.929317               1.000000   \n",
       "diff_gfg_editDistDP        0.963715               0.925329   \n",
       "\n",
       "                       diff_gfg_editDistDP  \n",
       "diff_jaccard                      0.963715  \n",
       "diff_sk_cosine_tf_idf             0.925329  \n",
       "diff_gfg_editDistDP               1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 836 entries, 0 to 835\n",
      "Data columns (total 3 columns):\n",
      "diff_jaccard             836 non-null float64\n",
      "diff_sk_cosine_tf_idf    836 non-null float64\n",
      "diff_gfg_editDistDP      836 non-null float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 19.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the quintiles - do not re-run that cell or it will crash!\n",
    "for m in s['metrics']:\n",
    "    for qtr in s['list_qtr'][s['lag']:]:\n",
    "        metric_scores[m][qtr] = post_processing.make_quintiles(metric_scores[m][qtr], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique cik set()\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: Verify that there are no CIK left for which we do not have stock prices.\n",
    "pnf = []\n",
    "for m in s['metrics']:\n",
    "    for qtr in s['list_qtr'][s['lag']:]:\n",
    "        for l in s['bin_labels']:\n",
    "            for cik in metric_scores[m][qtr][l]:\n",
    "                _, _, flag_price_found = post_processing.get_share_price(cik, qtr, lookup, stock_data)\n",
    "                if not flag_price_found:\n",
    "                    print(\"[WARNING] [{}] No stock data for {} during {}\".format(m, cik, qtr))\n",
    "                    pnf.append(cik)\n",
    "print(\"Unique cik\", set(pnf))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metric_scores['diff_jaccard'][(2013, 1)]  # After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_values = post_processing.initialize_portfolio(metric_scores, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pf_values['diff_jaccard'][(2013, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_values = post_processing.build_portfolio(pf_values, lookup, stock_data, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_processing.check_pf_value(pf_values, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf_values['diff_jaccard'][(2013, 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the data to postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = psycopg2.connect(host=\"localhost\", dbname=\"postgres\", user=\"postgres\", password=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cik_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cik_scores[851968][(2013, 1)].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_cik_scores = (('CIK', 'integer'), ('QTR', 'text'), \n",
    "                     ('METRIC', 'text'), ('SCORE', 'float'),\n",
    "                     ('TYPE', 'text'), ('PUBLISHED', 'date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/42 [00:00<00:03, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deleted table cik_scores\n",
      "[INFO] Creating the following table:\n",
      "CREATE TABLE cik_scores(IDX integer PRIMARY KEY,CIK integer,QTR text,METRIC text,SCORE float,TYPE text,PUBLISHED date)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:03<00:00, 12.30it/s]\n"
     ]
    }
   ],
   "source": [
    "postgres.cik_scores_to_postgres(connector, cik_scores, header_cik_scores, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = postgres.retrieve_cik_scores(connector, 851968, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[851968][(2013, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I.1. Push to csv\n",
    "path = s['path_output_folder']\n",
    "path_metric_scores = os.path.join(path, 'ms.csv')\n",
    "header_metric_score = (('METRIC', 'text'),  ('QUARTER', 'text'),\n",
    "                    ('QUINTILE', 'text'), ('CIK', 'integer'), \n",
    "                    ('SECTION', 'text'), ('SCORE', 'float'))\n",
    "with open(path_metric_scores, 'w') as f:\n",
    "    out = csv.writer(f, delimiter=';')\n",
    "    out.writerow(['IDX'] + [h[0] for h in header_metric_score])\n",
    "    c = 0\n",
    "    for m in metric_scores:\n",
    "        for qtr in metric_scores[m]:\n",
    "            for l in metric_scores[m][qtr]:\n",
    "                for cik in metric_scores[m][qtr][l]:\n",
    "                    #sections = [section for section in metric_scores[m][qtr][l][cik] if section != '0' and section != 'total']\n",
    "                    for section in metric_scores[m][qtr][l][cik]:\n",
    "                        v = metric_scores[m][qtr][l][cik][section]\n",
    "                        out.writerow([c, m, qtr, l, cik, section, v])\n",
    "                        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Deleted table metric_scores\n",
      "[INFO] Creating the following table:\n",
      "CREATE TABLE metric_scores(IDX integer PRIMARY KEY,METRIC text,QUARTER text,QUINTILE text,CIK integer,SECTION text,SCORE float)\n"
     ]
    }
   ],
   "source": [
    "# I.2. Move the csv to postgres\n",
    "postgres.csv_to_postgres(connector, 'metric_scores', header_metric_score, path_metric_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II. Sanity check: retrieve the data and compare to existing values\n",
    "ms = postgres.retrieve_ms_values_data(connector, path_metric_scores, s)\n",
    "assert ms == metric_scores\n",
    "del ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf_values['diff_jaccard'][(2013, 1)]['incoming_compo']['Q1'][49196]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = s['path_output_folder']\n",
    "header_pf_values1 = (('METRIC', 'text'),  ('QUARTER', 'text'),\n",
    "                    ('SECTION', 'text'), ('QUINTILE', 'text'),\n",
    "                    ('CIK', 'integer'), ('TICKER', 'text'),\n",
    "                    ('ASK', 'float'), ('MARKET_CAP', 'bigint'),\n",
    "                    ('SHARE_COUNT', 'float'), ('VALUE', 'float'),\n",
    "                    ('RATIO_PF_VALUE', 'float'))\n",
    "header_pf_values2 = (('METRIC', 'text'),  ('QUARTER', 'text'),\n",
    "                    ('SECTION', 'text'), ('QUINTILE', 'text'),\n",
    "                    ('PF_VALUE', 'float'))\n",
    "\n",
    "path1 = os.path.join(path, 'pf_values1.csv')\n",
    "# I.1. Dump to csv all the CIK info\n",
    "with open(path1, 'w') as f:\n",
    "    out = csv.writer(f, delimiter=';')\n",
    "    out.writerow(['IDX'] + [h[0] for h in header_pf_values1])\n",
    "    c = 0  # Primary key counter\n",
    "    for m in pf_values:\n",
    "        for qtr in pf_values[m]:\n",
    "            for section in ['incoming_compo', 'new_compo']:\n",
    "                for l in pf_values[m][qtr][section]:\n",
    "                    for cik in pf_values[m][qtr][section][l]:\n",
    "                        v = pf_values[m][qtr][section][l][cik]\n",
    "                        out.writerow([c, m, qtr, section, l, cik, *v])\n",
    "                        c += 1\n",
    "\n",
    "# I.2. Dump to csv all the pf values \n",
    "path2 = os.path.join(path, 'pf_values2.csv')\n",
    "with open(path2, 'w') as f:\n",
    "    out = csv.writer(f, delimiter=';')\n",
    "    out.writerow(['IDX'] + [h[0] for h in header_pf_values2])\n",
    "    c = 0  # Primary key counter\n",
    "    for m in pf_values:\n",
    "        for qtr in pf_values[m]:\n",
    "            for section in ['incoming_value', 'new_value']:\n",
    "                for l in pf_values[m][qtr][section]:\n",
    "                    v = pf_values[m][qtr][section][l]\n",
    "                    out.writerow([c, m, qtr, section, l, v])\n",
    "                    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I.3. CSV -> Postgres\n",
    "postgres.csv_to_postgres(connector, 'pf_values_compo', header_pf_values1, path1)\n",
    "postgres.csv_to_postgres(connector, 'pf_values_value', header_pf_values2, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II. Sanity check: retrieve the data and compare to existing values\n",
    "pf = postgres.retrieve_pf_values_data(connector, path1, path2, s)\n",
    "assert pf == pf_values\n",
    "del pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = [0.7, 1.5]\n",
    "fig, ax = plt.subplots(len(s['diff_metrics']), len(index_data), figsize=(15, 10))\n",
    "for idx_x, m in enumerate(s['diff_metrics']):\n",
    "    for idx_y, index_name in enumerate(index_data):\n",
    "        benchmark, bin_data = display.diff_vs_benchmark_ns(pf_values, index_name, index_data, m, s, norm_by_index=True)\n",
    "        display.update_ax_diff_vs_benchmark(ax[idx_x, idx_y], benchmark, bin_data, index_name, s, ylim, m)\n",
    "\n",
    "start = s['time_range'][0]   \n",
    "end = s['time_range'][1]\n",
    "plt.savefig(os.path.join(s['path_output_folder'], '{}Q{}_{}Q{}_{}_{}_sw-{}_lem-{}.png'\n",
    "                         .format(str(start[0])[2:], start[1], \n",
    "                                 str(end[0])[2:], end[1],\n",
    "                                 s['differentiation_mode'][0], s['pf_balancing'][0],\n",
    "                                 int(s['stop_words']), int(s['lemmatize']))))\n",
    "if display.run_from_ipython():\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'RUT'\n",
    "diff_method = 'diff_sk_cosine_tf_idf'\n",
    "diff_method = 'diff_jaccard'\n",
    "# diff_method='diff_gfg_editDistDP'\n",
    "benchmark, bin_data = display.diff_vs_benchmark_ns(pf_values, index_name, index_data, diff_method, s, norm_by_index=True)\n",
    "display.plot_diff_vs_benchmark(benchmark, bin_data, index_name, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For a given ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics vs stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik = 851968\n",
    "ticker = lookup[cik]\n",
    "start_date = qtrs.qtr_to_day(s['time_range'][0], 'first', date_format='datetime')\n",
    "stop_date = qtrs.qtr_to_day(s['time_range'][1], 'last', date_format='datetime')\n",
    "\n",
    "extracted_stock_data = {k: v for k, v in stock_data[ticker].items() if start_date <= k <= stop_date}\n",
    "#print(extracted_data)\n",
    "extracted_cik_scores = cik_scores[cik]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_stock_data = {k: v for k, v in stock_data[ticker].items() if start_date <= k <= stop_date}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "benchmark, metric_data = display.diff_vs_stock(extracted_cik_scores, extracted_stock_data, ticker, s, method='diff')\n",
    "display.plot_diff_vs_stock(benchmark, metric_data, ticker, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment vs stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark, metric_data = display.diff_vs_stock(extracted_cik_scores, extracted_stock_data, ticker, s, method='sentiment')\n",
    "display.plot_diff_vs_stock(benchmark, metric_data, ticker, s, method='sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
