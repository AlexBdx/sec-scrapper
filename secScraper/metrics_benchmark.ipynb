{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from secScraper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_0 = \"China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\"\n",
    "document_1 = \"At last, China seems serious about confronting an endemic problem: domestic violence and corruption.\"\n",
    "document_2 = \"Japan's prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.\"\n",
    "document_3 = \"Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.\"\n",
    "document_4 = \"What's the future of Abenomics? We asked Shinzo Abe for his views\"\n",
    "document_5 = \"Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble's value falls almost daily.\"\n",
    "document_6 = \"Vladimir Putin was found to be riding a horse, again, without a shirt on while hunting deer. Vladimir Putin always seems so serious about things - even riding horses.\"\n",
    "\n",
    "all_documents = [document_0, document_1, document_2, document_3, document_4, document_5, document_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    query = query.lower().split(\" \")\n",
    "    document = document.lower().split(\" \")\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    " \n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    count = tokenized_document.count(term)\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return 1 + math.log(count)\n",
    " \n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    " \n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    " \n",
    "def his_tfidf(str1, str2):\n",
    "    tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    tokenized_documents = [tokenize(d) for d in [str1, str2]]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    \n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return cosine_similarity(*tfidf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude\n",
    "\n",
    "def sk_cosine_tf(str1, str2):\n",
    "    #tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    #sklearn_tf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=True, sublinear_tf=True, tokenizer=tokenize)\n",
    "    sklearn_tf = TfidfVectorizer(norm='l2', use_idf=False)\n",
    "    sklearn_representation = sklearn_tf.fit_transform([str1, str2])\n",
    "    return cosine_similarity(*sklearn_representation.toarray())\n",
    "\n",
    "def sk_cosine_tf_idf(str1, str2):\n",
    "    #tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    #sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "    sklearn_tfidf = TfidfVectorizer(norm='l2', use_idf=True, sublinear_tf=True)\n",
    "    sklearn_representation = sklearn_tfidf.fit_transform([str1, str2])\n",
    "    return cosine_similarity(*sklearn_representation.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard\n",
    "my_jaccard = np.zeros((7, 7))\n",
    "his_jaccard = np.zeros((7, 7))\n",
    "\n",
    "# Cosine tf\n",
    "my_cosine_tf = np.zeros((7, 7))\n",
    "his_cosine_tf = np.zeros((7, 7))\n",
    "sklearn_cosine_tf = np.zeros((7, 7))\n",
    "\n",
    "# Cosine tf-idf\n",
    "my_cosine_tf_idf = np.zeros((7, 7))\n",
    "his_cosine_tf_idf = np.zeros((7, 7))\n",
    "sklearn_cosine_tf_idf = np.zeros((7, 7))\n",
    "\n",
    "# Min edits\n",
    "my_minEdit = np.zeros((7, 7))\n",
    "gfg_editDistDP = np.zeros((7, 7))\n",
    "\n",
    "# simpleEdit\n",
    "my_simpleEdit = np.zeros((7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"COMPARISONS\"\"\"\n",
    "sw = None\n",
    "for idx1, doc_1 in enumerate(all_documents):\n",
    "    for idx2, doc_2 in enumerate(all_documents):\n",
    "        doc1 = processing.normalize_text(doc_1)\n",
    "        doc2 = processing.normalize_text(doc_2)\n",
    "        \n",
    "        my_jaccard[idx1, idx2] = metrics.diff_jaccard(doc1, doc2)\n",
    "        his_jaccard[idx1, idx2] = jaccard_similarity(doc_1, doc_2)\n",
    "        \n",
    "        my_cosine_tf[idx1, idx2] = metrics.diff_sk_cosine_tf(doc_1, doc_2, sw)\n",
    "        his_cosine_tf[idx1, idx2] = 0\n",
    "        sklearn_cosine_tf[idx1, idx2] = sk_cosine_tf(doc_1, doc_2)\n",
    "        \n",
    "        my_cosine_tf_idf[idx1, idx2] = metrics.diff_sk_cosine_tf_idf(doc_1, doc_2, sw)\n",
    "        his_cosine_tf_idf[idx1, idx2] = his_tfidf(doc_1, doc_2)\n",
    "        sklearn_cosine_tf_idf[idx1, idx2] = sk_cosine_tf_idf(doc_1, doc_2)\n",
    "        \n",
    "        #my_minEdit[idx1, idx2] = metrics.diff_minEdit(doc1, doc2)\n",
    "        gfg_editDistDP[idx1, idx2] = metrics.diff_gfg_editDistDP(doc1, doc2)\n",
    "        \n",
    "        my_simpleEdit[idx1, idx2] = metrics.diff_simple(doc1, doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard correlation\n",
      "0.9977940992259946\n",
      "\n",
      "TF correlation\n",
      "nan\n",
      "0.9999999999999998\n",
      "nan\n",
      "\n",
      "TF-IDF correlation\n",
      "0.9917110417585476\n",
      "0.9996895393745222\n",
      "0.9942233407477253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/alex/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "# Inter correlations - pretty much all the same\n",
    "# Jaccard\n",
    "print(\"Jaccard correlation\")\n",
    "print(np.corrcoef(my_jaccard.reshape(-1,), his_jaccard.reshape(-1,))[0, 1])\n",
    "print()\n",
    "\n",
    "# TF\n",
    "print(\"TF correlation\")\n",
    "print(np.corrcoef(my_cosine_tf.reshape(-1,), his_cosine_tf.reshape(-1,))[0, 1])\n",
    "print(np.corrcoef(my_cosine_tf.reshape(-1,), sklearn_cosine_tf.reshape(-1,))[0, 1])\n",
    "print(np.corrcoef(his_cosine_tf.reshape(-1,), sklearn_cosine_tf.reshape(-1,))[0, 1])\n",
    "print()\n",
    "\n",
    "# TF-IDF\n",
    "print(\"TF-IDF correlation\")\n",
    "print(np.corrcoef(my_cosine_tf_idf.reshape(-1,), his_cosine_tf_idf.reshape(-1,))[0, 1])\n",
    "print(np.corrcoef(my_cosine_tf_idf.reshape(-1,), sklearn_cosine_tf_idf.reshape(-1,))[0, 1])\n",
    "print(np.corrcoef(his_cosine_tf_idf.reshape(-1,), sklearn_cosine_tf_idf.reshape(-1,))[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9701424599362335\n",
      "0.9936895911613337\n",
      "nan\n",
      "0.9555059103135598\n",
      "0.9911887436381733\n"
     ]
    }
   ],
   "source": [
    "# Cross correlations\n",
    "print(np.corrcoef(my_jaccard.reshape(-1,), my_cosine_tf.reshape(-1,))[0, 1])\n",
    "print(np.corrcoef(my_jaccard.reshape(-1,), my_cosine_tf_idf.reshape(-1,))[0, 1])\n",
    "print(np.corrcoef(my_jaccard.reshape(-1,), my_minEdit.reshape(-1,))[0, 1])\n",
    "print(np.corrcoef(my_jaccard.reshape(-1,), gfg_editDistDP.reshape(-1,))[0, 1])\n",
    "print(np.corrcoef(my_jaccard.reshape(-1,), my_simpleEdit.reshape(-1,))[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01 s ± 15.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "doc1 = processing.normalize_text(document_1, rm_stop_words=True, lemmatize=True)\n",
    "doc2 = processing.normalize_text(document_2, rm_stop_words=True, lemmatize=True)\n",
    "%timeit metrics.diff_gfg_editDistDP(doc1*n, doc2*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
